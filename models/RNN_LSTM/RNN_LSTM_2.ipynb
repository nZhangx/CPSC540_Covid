{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN with LSTM cell\n",
    "* Architecture reference from https://arxiv.org/pdf/2004.00959.pdf\n",
    "* Trained on all countries data\n",
    "\n",
    "(Something new: Maybe decrease number of classifier by grouping countries in the same GDP/ bracket)\n",
    "\n",
    "#### Data partition\n",
    "* Test data is the last 10 days\n",
    "* Validation 10 days before last 10 days\n",
    "* Data logged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "#import writer\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "#import \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data in\n",
    "csse_country = \"CPSC540_Covid/active_data/csse_world_normalized.csv\"\n",
    "df_csse = pd.read_csv(csse_country)\n",
    "df_csse.head(10)\n",
    "#list(df_csse.columns.values.tolist()) \n",
    "countries = df_csse[\"Country/Region\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171, 81)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries\n",
    "df_csse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if use grouping from k-means? or hierarchical clustering to decide which countries are in the same bracket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "(171, 5)\n",
      "(171, 5)\n",
      "(171, 69)\n"
     ]
    }
   ],
   "source": [
    "validation_d = 5\n",
    "test_d = 5\n",
    "\n",
    "# Convert to numpy array\n",
    "\n",
    "cases = df_csse.loc[:,'1/22/20':]\n",
    "casesA = cases.to_numpy()\n",
    "casesA\n",
    "count, days = casesA.shape\n",
    "\n",
    "print(days)\n",
    "# last 10 days for test\n",
    "\n",
    "case_test = casesA[:,-test_d:]\n",
    "case_validation = casesA[:,days-validation_d-test_d:days-test_d]\n",
    "case_train = casesA[:,:days-validation_d-test_d]\n",
    "\n",
    "print(case_test.shape)\n",
    "print(case_validation.shape)\n",
    "print(case_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2215155850>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot \n",
    "#epoch = np.linspace(1, len(trainErr), len(trainErr))\n",
    "\n",
    "cases.loc[50,:].plot.line()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.41700405e+01, 1.82186235e+01, 1.82186235e+01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.69184222e+00, 3.38368444e+00, 3.38368444e+00],\n",
       "       ...,\n",
       "       [3.02112385e-03, 3.02112385e-03, 6.04224770e-03, ...,\n",
       "        3.66960808e+02, 4.25703540e+02, 4.88911494e+02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.91862032e+05, 1.91862032e+05, 1.91862032e+05],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        9.77039570e+02, 9.77039570e+02, 9.77039570e+02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/home/nzhang/anaconda3/envs/neuralnet/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "/space/home/nzhang/anaconda3/envs/neuralnet/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/space/home/nzhang/anaconda3/envs/neuralnet/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[       -inf,        -inf,        -inf, ...,        -inf,\n",
       "               -inf,        -inf],\n",
       "       [       -inf,        -inf,        -inf, ...,  2.65112991,\n",
       "         2.90244434,  2.90244434],\n",
       "       [       -inf,        -inf,        -inf, ...,  0.52581801,\n",
       "         1.21896519,  1.21896519],\n",
       "       ...,\n",
       "       [-5.80212638, -5.80212638, -5.1089792 , ...,  5.90525505,\n",
       "         6.05374319,  6.19218148],\n",
       "       [       -inf,        -inf,        -inf, ..., 12.16453181,\n",
       "        12.16453181, 12.16453181],\n",
       "       [       -inf,        -inf,        -inf, ...,  6.88452715,\n",
       "         6.88452715,  6.88452715]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log transform\n",
    "case_test_log = np.log(case_test)\n",
    "case_validation_log = np.log(case_validation)\n",
    "case_train_log = np.log(case_train)\n",
    "case_train_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide data into training sets\n",
    "* specify number of days for train and prediction\n",
    "* offset the dataset so for example: [1,2,3,4] then get [1,2],[2,3] which predicts 3 and 4 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.00869841, 0.00869841, 0.00869841, 0.04349203,\n",
       "       0.04349203, 0.05219044, 0.05219044, 0.07828566, 0.07828566,\n",
       "       0.09568248, 0.09568248, 0.10438088, 0.10438088, 0.10438088,\n",
       "       0.13917451, 0.13917451, 0.18266655, 0.20006336])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "np.array(case_train[3,:])\n",
    "#total_days-pred_days-t_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_days = 40\n",
    "pred_days = test_d\n",
    "total_days = case_train.shape[1]\n",
    "\n",
    "country_train_X = []\n",
    "country_train_Y = []\n",
    "\n",
    "country_valid_X = []\n",
    "country_valid_Y = []\n",
    "\n",
    "country_test_X = []\n",
    "country_test_Y = []\n",
    "\n",
    "\n",
    "for i in range(len(countries)):\n",
    "    # split dataset\n",
    "    cur_c = [] # current country \n",
    "    out = []\n",
    "    \n",
    "    cur_v = [] # current country \n",
    "    out_v = []\n",
    "    \n",
    "    cur_t = [] # current country \n",
    "    out_t = []\n",
    "    \n",
    "    for j in range(total_days-pred_days-t_days): \n",
    "        \n",
    "        #print(j)\n",
    "        #print(case_train[i,j:j+t_days].shape)\n",
    "        #print(len(case_train[i,j+t_days]))\n",
    "        #cur_c.append(case_train[i,j:j+t_days])\n",
    "        #print(str(j))\n",
    "        #out.append(case_train[i,j+t_days:j+t_days+pred_days])\n",
    "        #print(len(cur_c))\n",
    "    \n",
    "        country_train_X.append(case_train[i,j:j+t_days])\n",
    "        country_train_Y.append(case_train[i,j+t_days:j+t_days+pred_days])\n",
    "    \n",
    "    country_valid_X.append(case_train[i,-t_days:])\n",
    "    country_valid_Y.append(case_validation[i])\n",
    "    \n",
    "    country_test_X.append(casesA[i,days-test_d-t_days:days-test_d])\n",
    "    country_test_Y.append(case_test[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dimension\n",
    "\n",
    "#len(country_test_X[33])\n",
    "total_days-pred_days-test_d\n",
    "pred_days\n",
    "test_d\n",
    "total_days\n",
    "case_train[0,29:64].shape\n",
    "case_train[0,0:40].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(country_train_X[1])\n",
    "len(country_train_Y[1])\n",
    "len(country_train_Y[1][0])\n",
    "#len(country_test_Y[1])\n",
    "#a = torch.tensor(country_train_X[10]).unsqueeze_(1).unsqueeze_(1)\n",
    "#torch.max(torch.tensor(country_train_X[2])).item()\n",
    "#torch.randn(a.shape[0],a.shape[1],a.shape[2])*0.05\n",
    "#a+a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data size & index\n",
    "The first axis is the sequence itself, the second indexes instances in the mini-batch, and the third indexes elements of the input. We haven’t discussed mini-batching, so lets just ignore that and assume we will always have just 1 dimension on the second axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2221ab3650>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized hyperparameters\n",
    "|Parameter |Description|Distribution/Selection|Values|\n",
    "|----------|------------|----------------------|---------|\n",
    "|Learning rate|Minimum learning rate| Log uniform|1e-1 to 1e-7|\n",
    "|Hidden layers|Number of layers in the network|Discrete numeric|1 to 20|\n",
    "|Hidden state|Number of memory cell in each layer|Discrete numeric|1 to 200|\n",
    "|Activation|Activation in each layer|Category|{ReLu,sigmoid,tanh}|\n",
    "|Batch size|Batch size during training|Discrete numeric|1 to 10|\n",
    "|Dropout|Dropout size before dense layer|Log uniform|0 to 0.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ./outputs/mock/training/ exists\n"
     ]
    }
   ],
   "source": [
    "# Parameter\n",
    "num_epochs = 100 # max\n",
    "batch_size = 1\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "learning_rate = 0.01 # to be changed/optimized \n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9)\n",
    "output_directory = \"mock/\"\n",
    "output_file_path = \"mock/Train\"\n",
    "input_dim = t_days\n",
    "hidden_dim = 2\n",
    "hidden_state = 100\n",
    "output_dim= pred_days\n",
    "num_layers=2\n",
    "lstm_input_size = t_days\n",
    "#model = LSTM(lstm_input_size, hidden_dim, batch_size=batch_size, output_dim=output_dim, num_layers=num_layers)\n",
    "\n",
    "model_name = \"mock\"\n",
    "output_file_path = \"./outputs/\" + model_name + \"/training/\"\n",
    "directory = os.path.dirname(output_file_path)\n",
    "if not os.path.exists(directory):\n",
    "    print(\"Creating directory %s\" % output_file_path)\n",
    "    os.makedirs(directory)\n",
    "else:\n",
    "     print(\"Directory %s exists\" % output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN structure\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    # you can also accept arguments in your model constructor\n",
    "    def __init__(self, data_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        input_size = data_size + hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, data, last_hidden):\n",
    "        input = torch.cat((data, last_hidden), 1)\n",
    "        hidden = self.i2h(input)\n",
    "        output = self.h2o(hidden)\n",
    "        return hidden, output\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM structure\n",
    "\n",
    "class LSTM(nn.Module):\n",
    " \n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=pred_days,\n",
    "                    num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    " \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers,dropout=0)\n",
    " \n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    " \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    " \n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
    "        \n",
    "        return y_pred.view(-1)\n",
    "\n",
    "#model = LSTM(lstm_input_size, h1, batch_size=num_train, output_dim=output_dim, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train_model(train_loader, test_loader, model, device, criterion, optimizer, num_epochs, output_directory,learning_rate,hidden_dim,num_layers):\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    model.train()\n",
    "    model.hidden = model.init_hidden() # LSTM hidden reinitialization\n",
    "    \n",
    "    #open files to log error\n",
    "    train_error = open(output_directory + \"training_error\"+ \"_\"+str(learning_rate)+ \"_\"+ str(hidden_dim) + \"_\"+ str(num_layers) +\".txt\", \"a\")\n",
    "    test_error = open(output_directory + \"validation_error\"+ \"_\"+str(learning_rate)+ \"_\"+ str(hidden_dim) + \"_\"+ str(num_layers) +\".txt\", \"a\")\n",
    "\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss_valid = float('inf')\n",
    "    best_epoch = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (seqs, labels) in enumerate(train_loader):\n",
    "            #print(seqs.shape)\n",
    "            # add gaussian noise to sequence\n",
    "            noiselevel = torch.max(torch.tensor(country_train_X[2])).item()/1000\n",
    "            noise = torch.randn(seqs.shape[0],seqs.shape[1],seqs.shape[2],seqs.shape[3])*noiselevel\n",
    "            #print(noise.shape)\n",
    "            seqs += noise # add gaussian noise\n",
    "            #print(seqs.shape)\n",
    "            seqs = seqs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            #seqs = seqs.squeeze()\n",
    "            #print(seqs.shape)\n",
    "            #print(model(seqs))\n",
    "            outputs = model(seqs) # error \n",
    "            loss = criterion(outputs, labels) # change input to \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #if (i+1) % 100 == 0:\n",
    "            #    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "            #           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "        #save training loss to file\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(\"%s, %s\" % (epoch, epoch_loss), file=train_error)\n",
    "        \n",
    "        #calculate test loss for epoch\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for i, (seqs, labels) in enumerate(test_loader):\n",
    "                x = seqs.to(device)\n",
    "                y = labels.to(device)\n",
    "                outputs = model(x)\n",
    "                #print(outputs.shape)\n",
    "                #print(y.shape)\n",
    "                loss = criterion(outputs, y)\n",
    "                test_loss += loss.item() \n",
    "                \n",
    "                # for metrics\n",
    "                #updateYlist(outputs,y)\n",
    "                \n",
    "\n",
    "        test_loss = test_loss / len(test_loader.dataset)\n",
    "        \n",
    "        #save outputs for epoch\n",
    "        print(\"%s, %s\" % (epoch, test_loss), file=test_error)\n",
    "        \n",
    "        # for each epoch, calculate metrics (f1, pr)\n",
    "        #calculateMetrics(outputs,y)\n",
    "        \n",
    "        if test_loss < best_loss_valid:\n",
    "            best_loss_valid = test_loss\n",
    "            best_epoch = epoch\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            print ('Saving the best model weights at Epoch [{}], Best Valid Loss: {:.4f}' \n",
    "                       .format(epoch+1, best_loss_valid))\n",
    "\n",
    "        \n",
    "    train_error.close()\n",
    "    test_error.close()\n",
    "\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    print(\"train error:\", epoch_loss)\n",
    "    return model, best_loss_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader, model, device,criterion):\n",
    "    #predictions = torch.zeros(0, numClass)\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (seqs, labels) in enumerate(test_loader):            \n",
    "            seqs = seqs.to(device)\n",
    "            y = labels.to(device).float()\n",
    "            print(y)\n",
    "            pred = model(seqs).float()\n",
    "            print(pred)\n",
    "            loss = criterion(pred, y)\n",
    "            test_loss += loss.item() \n",
    "            \n",
    "    return pred,test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dimension check\n",
    "a = torch.tensor(country_train_X[0]).unsqueeze_(2).unsqueeze_(2) #torch.Size([19, 30, 1, 1])\n",
    "torch.tensor(country_valid_X[0]).unsqueeze_(1).unsqueeze_(1).unsqueeze_(0).shape\n",
    "torch.tensor(country_valid_Y[0]).unsqueeze_(0).shape\n",
    "#torch.tensor(country_train_Y[0])\n",
    "torch.tensor(country_train_X[0])\n",
    "a.size(-1)\n",
    "torch.tensor(country_test_Y[0]).unsqueeze_(0).shape\n",
    "torch.tensor(country_valid_Y[0]).unsqueeze_(0).shape\n",
    "torch.tensor(country_train_Y[0]).unsqueeze_(1).shape\n",
    "#torch.tensor(country_valid_Y[0]).unsqueeze_(0).unsqueeze_(0).shape\n",
    "torch.tensor(country_train_Y[0]).unsqueeze_(1).shape\n",
    "torch.tensor(country_valid_Y[0]).unsqueeze_(0).shape\n",
    "torch.tensor(country_valid_X[0]).unsqueeze_(1).unsqueeze_(1).unsqueeze_(0).shape\n",
    "torch.tensor(country_train_X[0]).unsqueeze_(2).unsqueeze_(2).shape\n",
    "torch.tensor(country_valid_X[0]).unsqueeze_(1).unsqueeze_(1).unsqueeze_(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(40, 100, num_layers=2)\n",
      "  (linear): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/home/nzhang/anaconda3/envs/neuralnet/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1, 1, 5])) that is different to the input size (torch.Size([5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/space/home/nzhang/anaconda3/envs/neuralnet/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1, 5])) that is different to the input size (torch.Size([5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0484\n",
      "train error: 6.7692523542930635e-12\n"
     ]
    }
   ],
   "source": [
    "# test if it runs\n",
    "learning_rate = 0.01 # to be changed/optimized \n",
    "num_layers = 2\n",
    "hidden_dim = 100\n",
    "\n",
    "net =  LSTM(lstm_input_size, hidden_dim, batch_size=batch_size, output_dim=output_dim, num_layers=num_layers)\n",
    "print(net)\n",
    "model = net.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(country_train_X[0]).unsqueeze_(2).unsqueeze_(2).float(), torch.tensor(country_train_Y[0]).unsqueeze_(1).float())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# load validation\n",
    "valid_dataset = torch.utils.data.TensorDataset(torch.tensor(country_valid_X[0]).unsqueeze_(1).unsqueeze_(1).unsqueeze_(0).float(), torch.tensor(country_valid_Y[0]).unsqueeze_(0).float())\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# load test\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(country_test_X[0]).unsqueeze_(1).unsqueeze_(1).unsqueeze_(0).float(), torch.tensor(country_test_Y[0]).unsqueeze_(0).float())\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# mock trial\n",
    "err = train_model(train_loader, valid_loader, model, device, criterion,  optimizer, num_epochs, output_file_path,learning_rate,hidden_dim, num_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_model(test_loader,model,device,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(country_train_Y[0]).unsqueeze_(0).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ./outputs/Burundi/training/ exists\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0501\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0477\n",
      "train error: 1.2224617261756092e-09\n",
      "0.04767756909132004\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0494\n",
      "train error: 3.541498357594042e-10\n",
      "0.049432095140218735\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0493\n",
      "train error: 2.5686448685650437e-10\n",
      "0.049295779317617416\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0487\n",
      "train error: 1.1491109431181756e-10\n",
      "0.04872298240661621\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0494\n",
      "train error: 8.300809974277901e-11\n",
      "0.04938066005706787\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0487\n",
      "train error: 1.7222555422095078e-11\n",
      "0.048726968467235565\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0488\n",
      "train error: 1.6572948841906663e-11\n",
      "0.04883439466357231\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [4], Best Valid Loss: 0.0495\n",
      "train error: 5.401032772697391e-12\n",
      "0.049492862075567245\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0493\n",
      "train error: 2.503130761567571e-12\n",
      "0.04933328554034233\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0490\n",
      "train error: 2.5456427443615605e-12\n",
      "0.04901643842458725\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0484\n",
      "train error: 2.603972118203934e-13\n",
      "0.04838564619421959\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0489\n",
      "train error: 3.613702713509708e-13\n",
      "0.04890589043498039\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0493\n",
      "train error: 9.75438709252295e-14\n",
      "0.04927739128470421\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0495\n",
      "train error: 7.451743931985022e-14\n",
      "0.049500491470098495\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0487\n",
      "train error: 3.3292098443362835e-14\n",
      "0.048727847635746\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0490\n",
      "train error: 3.00742508956613e-15\n",
      "0.04901957884430885\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0507\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0479\n",
      "train error: 4.056041981878105e-15\n",
      "0.04786182567477226\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0491\n",
      "train error: 2.0235676887490834e-15\n",
      "0.0490911491215229\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0494\n",
      "train error: 8.058033577090774e-16\n",
      "0.049364734441041946\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0495\n",
      "train error: 6.014227640797438e-16\n",
      "0.04947378486394882\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0478\n",
      "train error: 1.5493283117765923e-16\n",
      "0.047816064208745956\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0486\n",
      "train error: 9.469060519899475e-17\n",
      "0.048636820167303085\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0501\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0486\n",
      "train error: 4.278397375792202e-17\n",
      "0.048630643635988235\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0493\n",
      "train error: 2.376277564324899e-17\n",
      "0.04928424581885338\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0484\n",
      "train error: 1.6929817174961922e-17\n",
      "0.048399150371551514\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0478\n",
      "train error: 7.92093126477054e-10\n",
      "0.04778732731938362\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0555\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0502\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0493\n",
      "train error: 3.8749144836101407e-10\n",
      "0.04931342229247093\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0511\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0495\n",
      "train error: 2.4088489464108947e-10\n",
      "0.04947473108768463\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0442\n",
      "train error: 1.5307761698848954e-10\n",
      "0.044236090034246445\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0443\n",
      "train error: 6.022399780837111e-11\n",
      "0.04429503157734871\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0408\n",
      "train error: 2.1873631757870354e-11\n",
      "0.040791451930999756\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0704\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0518\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0487\n",
      "train error: 7.254850813522874e-12\n",
      "0.0487169548869133\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0472\n",
      "train error: 6.270835332120375e-12\n",
      "0.04717184603214264\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0519\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0494\n",
      "train error: 2.2655878005537386e-12\n",
      "0.049417536705732346\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0495\n",
      "train error: 1.2829210248668527e-12\n",
      "0.04949561879038811\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0495\n",
      "train error: 2.1535951484406209e-13\n",
      "0.04948103055357933\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0482\n",
      "train error: 2.4434027900749247e-13\n",
      "0.04816329479217529\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0582\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0504\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0492\n",
      "train error: 9.3128633037128e-14\n",
      "0.049158524721860886\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0530\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0494\n",
      "train error: 4.5348458223965005e-14\n",
      "0.04936130344867706\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0469\n",
      "train error: 2.2515604775494157e-14\n",
      "0.04686060547828674\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0404\n",
      "train error: 3.5665607704872375e-15\n",
      "0.04037705063819885\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0440\n",
      "train error: 3.268905760702291e-15\n",
      "0.043985091149806976\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0522\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0494\n",
      "train error: 1.909354063926334e-15\n",
      "0.04941600188612938\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0493\n",
      "train error: 1.0288252351688096e-15\n",
      "0.04929100349545479\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0483\n",
      "train error: 7.862159959792389e-16\n",
      "0.04826415330171585\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0565\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0501\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0493\n",
      "train error: 1.2566806829120595e-16\n",
      "0.04930327460169792\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0639\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0489\n",
      "train error: 3.914692726149739e-17\n",
      "0.04894004389643669\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0467\n",
      "train error: 3.313604262530599e-17\n",
      "0.04665930196642876\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0510\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0494\n",
      "train error: 2.6179417088225853e-17\n",
      "0.04944324493408203\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0464\n",
      "train error: 1.771061195445542e-17\n",
      "0.0464303158223629\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0623\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0604\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0587\n",
      "Saving the best model weights at Epoch [4], Best Valid Loss: 0.0573\n",
      "Saving the best model weights at Epoch [5], Best Valid Loss: 0.0561\n",
      "Saving the best model weights at Epoch [6], Best Valid Loss: 0.0551\n",
      "Saving the best model weights at Epoch [7], Best Valid Loss: 0.0543\n",
      "Saving the best model weights at Epoch [8], Best Valid Loss: 0.0536\n",
      "Saving the best model weights at Epoch [9], Best Valid Loss: 0.0530\n",
      "Saving the best model weights at Epoch [10], Best Valid Loss: 0.0526\n",
      "Saving the best model weights at Epoch [11], Best Valid Loss: 0.0521\n",
      "Saving the best model weights at Epoch [12], Best Valid Loss: 0.0518\n",
      "Saving the best model weights at Epoch [13], Best Valid Loss: 0.0515\n",
      "Saving the best model weights at Epoch [14], Best Valid Loss: 0.0512\n",
      "Saving the best model weights at Epoch [15], Best Valid Loss: 0.0510\n",
      "Saving the best model weights at Epoch [16], Best Valid Loss: 0.0508\n",
      "Saving the best model weights at Epoch [17], Best Valid Loss: 0.0507\n",
      "Saving the best model weights at Epoch [18], Best Valid Loss: 0.0505\n",
      "Saving the best model weights at Epoch [19], Best Valid Loss: 0.0504\n",
      "Saving the best model weights at Epoch [20], Best Valid Loss: 0.0503\n",
      "Saving the best model weights at Epoch [21], Best Valid Loss: 0.0502\n",
      "Saving the best model weights at Epoch [22], Best Valid Loss: 0.0501\n",
      "Saving the best model weights at Epoch [23], Best Valid Loss: 0.0501\n",
      "Saving the best model weights at Epoch [24], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [25], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [26], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [27], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [28], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [29], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [30], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [31], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [32], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [33], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [34], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [35], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [36], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [37], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [38], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [39], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [40], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [41], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [42], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [43], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [44], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [45], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [46], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [47], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [48], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [49], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [50], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [51], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [52], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [53], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [54], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [55], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [56], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [57], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [58], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [59], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [60], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [61], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [62], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [63], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [64], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [65], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [66], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [67], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [68], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [69], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [70], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [71], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [72], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [73], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [74], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [75], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [76], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [77], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [78], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [79], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [80], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [81], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [82], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [83], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [84], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [85], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [86], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [87], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [88], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [89], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [99], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [100], Best Valid Loss: 0.0495\n",
      "train error: 6.158312150159051e-10\n",
      "0.049508485943078995\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0435\n",
      "train error: 4.997964481515127e-10\n",
      "0.04354187846183777\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0443\n",
      "train error: 2.1119802825000922e-10\n",
      "0.044324249029159546\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0467\n",
      "train error: 1.0036277104524816e-10\n",
      "0.046685393899679184\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0572\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0562\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0553\n",
      "Saving the best model weights at Epoch [4], Best Valid Loss: 0.0546\n",
      "Saving the best model weights at Epoch [5], Best Valid Loss: 0.0539\n",
      "Saving the best model weights at Epoch [6], Best Valid Loss: 0.0534\n",
      "Saving the best model weights at Epoch [7], Best Valid Loss: 0.0529\n",
      "Saving the best model weights at Epoch [8], Best Valid Loss: 0.0525\n",
      "Saving the best model weights at Epoch [9], Best Valid Loss: 0.0521\n",
      "Saving the best model weights at Epoch [10], Best Valid Loss: 0.0518\n",
      "Saving the best model weights at Epoch [11], Best Valid Loss: 0.0516\n",
      "Saving the best model weights at Epoch [12], Best Valid Loss: 0.0513\n",
      "Saving the best model weights at Epoch [13], Best Valid Loss: 0.0511\n",
      "Saving the best model weights at Epoch [14], Best Valid Loss: 0.0509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model weights at Epoch [15], Best Valid Loss: 0.0508\n",
      "Saving the best model weights at Epoch [16], Best Valid Loss: 0.0506\n",
      "Saving the best model weights at Epoch [17], Best Valid Loss: 0.0505\n",
      "Saving the best model weights at Epoch [18], Best Valid Loss: 0.0504\n",
      "Saving the best model weights at Epoch [19], Best Valid Loss: 0.0503\n",
      "Saving the best model weights at Epoch [20], Best Valid Loss: 0.0502\n",
      "Saving the best model weights at Epoch [21], Best Valid Loss: 0.0501\n",
      "Saving the best model weights at Epoch [22], Best Valid Loss: 0.0501\n",
      "Saving the best model weights at Epoch [23], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [24], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [25], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [26], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [27], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [28], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [29], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [30], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [31], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [32], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [33], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [34], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [35], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [36], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [37], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [38], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [39], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [40], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [41], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [42], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [43], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [44], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [45], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [46], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [47], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [48], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [49], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [50], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [51], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [52], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [53], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [54], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [55], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [56], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [57], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [58], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [59], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [60], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [61], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [62], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [63], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [64], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [65], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [66], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [67], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [68], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [69], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [70], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [71], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [72], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [73], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [74], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [75], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [76], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [77], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [78], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [79], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [80], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [81], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [82], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [83], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [84], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [85], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [86], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [87], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [88], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [89], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [92], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [93], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [100], Best Valid Loss: 0.0495\n",
      "train error: 6.217610830130023e-11\n",
      "0.04950837790966034\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0883\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0837\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0795\n",
      "Saving the best model weights at Epoch [4], Best Valid Loss: 0.0758\n",
      "Saving the best model weights at Epoch [5], Best Valid Loss: 0.0727\n",
      "Saving the best model weights at Epoch [6], Best Valid Loss: 0.0699\n",
      "Saving the best model weights at Epoch [7], Best Valid Loss: 0.0676\n",
      "Saving the best model weights at Epoch [8], Best Valid Loss: 0.0655\n",
      "Saving the best model weights at Epoch [9], Best Valid Loss: 0.0636\n",
      "Saving the best model weights at Epoch [10], Best Valid Loss: 0.0620\n",
      "Saving the best model weights at Epoch [11], Best Valid Loss: 0.0606\n",
      "Saving the best model weights at Epoch [12], Best Valid Loss: 0.0594\n",
      "Saving the best model weights at Epoch [13], Best Valid Loss: 0.0583\n",
      "Saving the best model weights at Epoch [14], Best Valid Loss: 0.0573\n",
      "Saving the best model weights at Epoch [15], Best Valid Loss: 0.0565\n",
      "Saving the best model weights at Epoch [16], Best Valid Loss: 0.0557\n",
      "Saving the best model weights at Epoch [17], Best Valid Loss: 0.0550\n",
      "Saving the best model weights at Epoch [18], Best Valid Loss: 0.0544\n",
      "Saving the best model weights at Epoch [19], Best Valid Loss: 0.0539\n",
      "Saving the best model weights at Epoch [20], Best Valid Loss: 0.0534\n",
      "Saving the best model weights at Epoch [21], Best Valid Loss: 0.0530\n",
      "Saving the best model weights at Epoch [22], Best Valid Loss: 0.0526\n",
      "Saving the best model weights at Epoch [23], Best Valid Loss: 0.0523\n",
      "Saving the best model weights at Epoch [24], Best Valid Loss: 0.0520\n",
      "Saving the best model weights at Epoch [25], Best Valid Loss: 0.0517\n",
      "Saving the best model weights at Epoch [26], Best Valid Loss: 0.0515\n",
      "Saving the best model weights at Epoch [27], Best Valid Loss: 0.0513\n",
      "Saving the best model weights at Epoch [28], Best Valid Loss: 0.0511\n",
      "Saving the best model weights at Epoch [29], Best Valid Loss: 0.0509\n",
      "Saving the best model weights at Epoch [30], Best Valid Loss: 0.0508\n",
      "Saving the best model weights at Epoch [31], Best Valid Loss: 0.0507\n",
      "Saving the best model weights at Epoch [32], Best Valid Loss: 0.0505\n",
      "Saving the best model weights at Epoch [33], Best Valid Loss: 0.0504\n",
      "Saving the best model weights at Epoch [34], Best Valid Loss: 0.0503\n",
      "Saving the best model weights at Epoch [35], Best Valid Loss: 0.0503\n",
      "Saving the best model weights at Epoch [36], Best Valid Loss: 0.0502\n",
      "Saving the best model weights at Epoch [37], Best Valid Loss: 0.0501\n",
      "Saving the best model weights at Epoch [38], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [39], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [40], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [41], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [42], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [43], Best Valid Loss: 0.0498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model weights at Epoch [44], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [45], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [46], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [47], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [48], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [49], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [50], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [51], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [52], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [53], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [54], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [55], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [56], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [57], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [58], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [59], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [60], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [61], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [62], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [63], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [64], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [65], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [66], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [67], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [68], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [69], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [70], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [71], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [72], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [73], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [74], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [75], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [76], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [77], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [78], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [79], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [80], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [81], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [82], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [83], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [84], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [85], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [86], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [87], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [88], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [89], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [90], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [91], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [92], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [93], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [94], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [95], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [96], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [97], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [98], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [99], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [100], Best Valid Loss: 0.0495\n",
      "train error: 1.7261530619947184e-11\n",
      "0.04950898513197899\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0429\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0428\n",
      "train error: 1.3020707703588022e-11\n",
      "0.04283785820007324\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0502\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [4], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [5], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [6], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [7], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [8], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [9], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [10], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [11], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [12], Best Valid Loss: 0.0494\n",
      "Saving the best model weights at Epoch [13], Best Valid Loss: 0.0494\n",
      "Saving the best model weights at Epoch [14], Best Valid Loss: 0.0494\n",
      "train error: 6.716355481032703e-12\n",
      "0.04944271594285965\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0564\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0556\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0548\n",
      "Saving the best model weights at Epoch [4], Best Valid Loss: 0.0542\n",
      "Saving the best model weights at Epoch [5], Best Valid Loss: 0.0536\n",
      "Saving the best model weights at Epoch [6], Best Valid Loss: 0.0531\n",
      "Saving the best model weights at Epoch [7], Best Valid Loss: 0.0527\n",
      "Saving the best model weights at Epoch [8], Best Valid Loss: 0.0524\n",
      "Saving the best model weights at Epoch [9], Best Valid Loss: 0.0520\n",
      "Saving the best model weights at Epoch [10], Best Valid Loss: 0.0517\n",
      "Saving the best model weights at Epoch [11], Best Valid Loss: 0.0515\n",
      "Saving the best model weights at Epoch [12], Best Valid Loss: 0.0513\n",
      "Saving the best model weights at Epoch [13], Best Valid Loss: 0.0511\n",
      "Saving the best model weights at Epoch [14], Best Valid Loss: 0.0509\n",
      "Saving the best model weights at Epoch [15], Best Valid Loss: 0.0507\n",
      "Saving the best model weights at Epoch [16], Best Valid Loss: 0.0506\n",
      "Saving the best model weights at Epoch [17], Best Valid Loss: 0.0505\n",
      "Saving the best model weights at Epoch [18], Best Valid Loss: 0.0504\n",
      "Saving the best model weights at Epoch [19], Best Valid Loss: 0.0503\n",
      "Saving the best model weights at Epoch [20], Best Valid Loss: 0.0502\n",
      "Saving the best model weights at Epoch [21], Best Valid Loss: 0.0501\n",
      "Saving the best model weights at Epoch [22], Best Valid Loss: 0.0501\n",
      "Saving the best model weights at Epoch [23], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [24], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [25], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [26], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [27], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [28], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [29], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [30], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [31], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [32], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [33], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [34], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [35], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [36], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [37], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [38], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [39], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [40], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [41], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [42], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [43], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [44], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [45], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [46], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [47], Best Valid Loss: 0.0495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model weights at Epoch [48], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [49], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [50], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [51], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [52], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [53], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [54], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [55], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [56], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [57], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [58], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [59], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [60], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [61], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [62], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [63], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [64], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [65], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [66], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [67], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [68], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [69], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [70], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [71], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [72], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [73], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [74], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [75], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [76], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [77], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [78], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [79], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [80], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [81], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [82], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [83], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [84], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [85], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [86], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [87], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [88], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [89], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [90], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [91], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [92], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [93], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [94], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [95], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [96], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [97], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [98], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [99], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [100], Best Valid Loss: 0.0495\n",
      "train error: 3.0885315859271634e-12\n",
      "0.04950834438204765\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0585\n",
      "Saving the best model weights at Epoch [2], Best Valid Loss: 0.0576\n",
      "Saving the best model weights at Epoch [3], Best Valid Loss: 0.0567\n",
      "Saving the best model weights at Epoch [4], Best Valid Loss: 0.0559\n",
      "Saving the best model weights at Epoch [5], Best Valid Loss: 0.0552\n",
      "Saving the best model weights at Epoch [6], Best Valid Loss: 0.0546\n",
      "Saving the best model weights at Epoch [7], Best Valid Loss: 0.0540\n",
      "Saving the best model weights at Epoch [8], Best Valid Loss: 0.0535\n",
      "Saving the best model weights at Epoch [9], Best Valid Loss: 0.0531\n",
      "Saving the best model weights at Epoch [10], Best Valid Loss: 0.0527\n",
      "Saving the best model weights at Epoch [11], Best Valid Loss: 0.0524\n",
      "Saving the best model weights at Epoch [12], Best Valid Loss: 0.0521\n",
      "Saving the best model weights at Epoch [13], Best Valid Loss: 0.0518\n",
      "Saving the best model weights at Epoch [14], Best Valid Loss: 0.0516\n",
      "Saving the best model weights at Epoch [15], Best Valid Loss: 0.0513\n",
      "Saving the best model weights at Epoch [16], Best Valid Loss: 0.0511\n",
      "Saving the best model weights at Epoch [17], Best Valid Loss: 0.0510\n",
      "Saving the best model weights at Epoch [18], Best Valid Loss: 0.0508\n",
      "Saving the best model weights at Epoch [19], Best Valid Loss: 0.0507\n",
      "Saving the best model weights at Epoch [20], Best Valid Loss: 0.0506\n",
      "Saving the best model weights at Epoch [21], Best Valid Loss: 0.0504\n",
      "Saving the best model weights at Epoch [22], Best Valid Loss: 0.0503\n",
      "Saving the best model weights at Epoch [23], Best Valid Loss: 0.0503\n",
      "Saving the best model weights at Epoch [24], Best Valid Loss: 0.0502\n",
      "Saving the best model weights at Epoch [25], Best Valid Loss: 0.0501\n",
      "Saving the best model weights at Epoch [26], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [27], Best Valid Loss: 0.0500\n",
      "Saving the best model weights at Epoch [28], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [29], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [30], Best Valid Loss: 0.0499\n",
      "Saving the best model weights at Epoch [31], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [32], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [33], Best Valid Loss: 0.0498\n",
      "Saving the best model weights at Epoch [34], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [35], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [36], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [37], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [38], Best Valid Loss: 0.0497\n",
      "Saving the best model weights at Epoch [39], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [40], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [41], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [42], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [43], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [44], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [45], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [46], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [47], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [48], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [49], Best Valid Loss: 0.0496\n",
      "Saving the best model weights at Epoch [50], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [51], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [52], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [53], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [54], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [55], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [56], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [57], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [58], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [59], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [60], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [61], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [62], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [63], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [64], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [65], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [66], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [67], Best Valid Loss: 0.0495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model weights at Epoch [68], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [69], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [70], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [71], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [72], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [73], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [74], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [75], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [76], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [77], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [78], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [79], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [80], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [81], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [82], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [83], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [84], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [85], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [86], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [87], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [88], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [89], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [90], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [91], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [92], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [93], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [94], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [95], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [96], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [97], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [98], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [99], Best Valid Loss: 0.0495\n",
      "Saving the best model weights at Epoch [100], Best Valid Loss: 0.0495\n",
      "train error: 1.8624260888202315e-12\n",
      "0.049508433789014816\n",
      "Saving the best model weights at Epoch [1], Best Valid Loss: 0.0193\n"
     ]
    }
   ],
   "source": [
    "# optimization grid\n",
    "lr_list = [0.1,0.01,0.001,0.005,1e-4,5e-4,1e-5,1e-6]\n",
    "hidden_lst = [1,2,3,4,5]\n",
    "hState_lst = [30,50,100,200,300]\n",
    "\n",
    "country_result = dict()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9)\n",
    "best_error = 1e10\n",
    "\n",
    "for i in range(len(countries)):\n",
    "    country_result[countries[i]] = [] # store best validation parameter, test loss, test result\n",
    "    best_error = 1e10\n",
    "    \n",
    "    best_param = [0,0,0]\n",
    "\n",
    "    learning_rate = lr_list[0] # to be changed/optimized \n",
    "    hidden_dim = hidden_lst[0]\n",
    "    hidden_state = hState_lst[0]\n",
    "    \n",
    "    \n",
    "    model_name = countries[i]\n",
    "    output_file_path = \"./outputs/\" + model_name + \"/training/\"\n",
    "    directory = os.path.dirname(output_file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        print(\"Creating directory %s\" % output_file_path)\n",
    "        os.makedirs(directory)\n",
    "    else:\n",
    "         print(\"Directory %s exists\" % output_file_path)\n",
    "\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(country_train_X[i]).unsqueeze_(2).unsqueeze_(2).float(), torch.tensor(country_train_Y[i]).unsqueeze_(1).float())\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # load validation\n",
    "    valid_dataset = torch.utils.data.TensorDataset(torch.tensor(country_valid_X[i]).unsqueeze_(1).unsqueeze_(1).unsqueeze_(0).float(), torch.tensor(country_valid_Y[i]).unsqueeze_(0).float())\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # load test\n",
    "    test_dataset = torch.utils.data.TensorDataset(torch.tensor(country_test_X[i]).unsqueeze_(1).unsqueeze_(1).unsqueeze_(0).float(), torch.tensor(country_test_Y[i]).unsqueeze_(0).float())\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    bestmodel = 0\n",
    "    \n",
    "    # optimize (learning rate, hidden layer, hidden state, activation)\n",
    "    for j in range(len(lr_list)):\n",
    "        for k in range(len(hidden_lst)): # hidden layer\n",
    "            for m in range(len(hState_lst)): # hidden state\n",
    "                # train\n",
    "                # specific parameters\n",
    "                learning_rate = lr_list[j]\n",
    "                hidden_dim = hState_lst[m]\n",
    "                num_layers = hidden_lst[k]\n",
    "                \n",
    "                net =  LSTM(lstm_input_size, hidden_dim, batch_size=batch_size, output_dim=output_dim, num_layers=num_layers)\n",
    "                #print(net)\n",
    "                model = net.to(device)\n",
    "                \n",
    "                #\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9)\n",
    "                # to be changed/optimized \n",
    "                \n",
    "                model, err = train_model(train_loader, valid_loader, model, device, criterion,  optimizer, num_epochs, output_file_path,learning_rate,hidden_dim,num_layers)\n",
    "                print(err)\n",
    "                if err < best_error:\n",
    "                    best_error = err\n",
    "                    best_param = [lr_list[j],hidden_lst[k],hState_lst[m]]\n",
    "                    bestmodel = model\n",
    "    \n",
    "    country_result[countries[i]].append(best_param)\n",
    "    \n",
    "    # test data\n",
    "    pred,final_loss = test_model(test_loader,bestmodel,device,criterion)\n",
    "    print(\"test loss: \",final_loss)\n",
    "    country_result[countries[i]].append([pred.tolist(),country_test_Y[i].tolist()])\n",
    "    country_result[countries[i]].append(final_loss)\n",
    "    torch.save(bestmodel.state_dict(), output_file_path+countries[i]+\".th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"country_summary.js\",'w+') as outfile:\n",
    "    json.dump(country_result, outfile , sort_keys = True , indent = 4, separators = (\",\",\": \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country_test_Y[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
